{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a0b1c2d3",
      "metadata": {
        "id": "a0b1c2d3"
      },
      "source": [
        "![image](https://github.com/nicolas-thiers/Mec-270-Lab/blob/main/Lab05/Images/logo-mecanica.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d6fe3a0",
      "metadata": {
        "id": "1d6fe3a0"
      },
      "source": [
        "# Laboratorio 5: Resolución de ecuaciones lineales"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "Este notebook repasa los **métodos numéricos directos e iterativos para encontrar la solución de sistemas de ecuaciones lineales**. Se incluye una breve revisión de la teoría y los algoritmos generales de cada método, seguida de ejercicios prácticos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "theory-review",
      "metadata": {
        "id": "theory-review"
      },
      "source": [
        "## 1. Repaso de la teoría"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "general-problem",
      "metadata": {
        "id": "general-problem"
      },
      "source": [
        "**Problema General:** Encontrar el vector $\\vec{x}$ que satisface el **sistema de ecuaciones lineales**: $A\\vec{x} = \\vec{b}$\n",
        "\n",
        "Aquí, $A$ es una matriz cuadrada de tamaño $n \\times n$ y $\\vec{b}$ es un vector conocido. La solución buscada es $\\vec{x}$. Resolver el sistema implica encontrar el vector $\\vec{x} = A^{-1}\\vec{b}$. Sin embargo, calcular la inversa de la matriz $A$ tiene un costo computacional elevado."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "direct-methods",
      "metadata": {
        "id": "direct-methods"
      },
      "source": [
        "### 1.1 Métodos Directos\n",
        "\n",
        "La idea principal de los métodos directos es **transformar el sistema de ecuaciones lineales original $A\\vec{x} = \\vec{b}$ en un sistema equivalente que sea más fácil de resolver**. Esto se logra pre-multiplicando (operando por la izquierda) por una matriz $P$ no singular: $P \\cdot A\\vec{x} = P \\cdot \\vec{b}$ . Dado que $P$ es no singular, $P^{-1}$ existe, y si $\\vec{z}$ es la solución del sistema transformado $M A \\vec{z} = M \\vec{b}$, entonces $\\vec{z} = A^{-1} \\vec{b} = \\vec{x}$, demostrando que la solución no cambia.\n",
        "\n",
        "**Transformaciones útiles** (pre-multiplicación, no modifican la solución):\n",
        "*   **Permutación:** Reordenamiento de las filas del sistema de ecuaciones lineales. Se realiza mediante una matriz de permutación $P$.\n",
        "*   **Escalamiento diagonal:** Multiplicación de cada fila por un escalar distinto de cero. Se realiza mediante una matriz diagonal $D$.\n",
        "\n",
        "**Sistemas Equivalentes Fáciles de Resolver**:\n",
        "*   **Diagonal:** La matriz asociada es diagonal.\n",
        "*   **Tri-diagonal:** La matriz asociada solo tiene elementos en la diagonal principal y en las diagonales inmediatamente superior e inferior.\n",
        "*   **Triangular superior:** La matriz asociada $U$ tiene ceros debajo de la diagonal principal. Un sistema $U\\vec{x} = \\vec{b}$ se resuelve eficientemente mediante **sustitución hacia atrás**. El algoritmo es:\n",
        "\n",
        "```\n",
        "  for j = n to 1:\n",
        "    if u_jj == 0 -> stop\n",
        "    x_j = b_j / u_jj\n",
        "    for i = 1 to j-1:\n",
        "      b_i = b_i - u_ij * x_j\n",
        "    end\n",
        "  end\n",
        "```\n",
        "*   **Triangular inferior:** La matriz asociada $L$ tiene ceros encima de la diagonal principal. Un sistema $L\\vec{x} = \\vec{b}$ se resuelve eficientemente mediante **sustitución hacia adelante**. El algoritmo es:\n",
        "\n",
        "```\n",
        "  for j = 1 to n:\n",
        "    if l_jj == 0 -> stop\n",
        "    x_j = b_j / l_jj\n",
        "    for i = j+1 to n:\n",
        "      b_i = b_i - l_ij * x_j\n",
        "    end\n",
        "  end\n",
        "```\n",
        "\n",
        "#### Eliminación Gaussiana y Factorización LU\n",
        "\n",
        "El objetivo es transformar la matriz $A$ en una matriz triangular superior mediante operaciones que reemplacen elementos por ceros. Estas operaciones se basan en la combinación lineal de filas y pueden representarse mediante **matrices de eliminación elemental** $M_k$. Una matriz $M_k$ puede transformar un vector columna $\\vec{a}$ en un vector con ceros a partir de la fila $k+1$.\n",
        "\n",
        "La **Eliminación Gaussiana** es un método para resolver $A\\vec{x} = \\vec{b}$ transformando el sistema original en un sistema triangular utilizando matrices de eliminación elemental. Aplica secuencialmente matrices de eliminación elemental a la matriz $A$ utilizando los vectores columna como pivotes. La secuencia de operaciones es:\n",
        "\n",
        "$$ A\\vec{x} = \\vec{b} \\Rightarrow M_1 A\\vec{x} = M_1 \\vec{b} \\Rightarrow M_2 M_1 A\\vec{x} = M_2 M_1 \\vec{b} \\Rightarrow \\dots \\Rightarrow M_n M_{n-1} \\dots M_1 A\\vec{x} = M_n M_{n-1} \\dots M_1 \\vec{b} $$\n",
        "\n",
        "Si definimos $M = M_n M_{n-1} \\dots M_1$, entonces $M A = U$, donde $U$ es una matriz triangular superior, y el sistema transformado es $U\\vec{x} = M\\vec{b}$. Este sistema triangular se resuelve luego mediante sustitución hacia atrás.\n",
        "\n",
        "La inversa de la matriz de transformación $M$ es $M^{-1} = L$, donde $L$ es una matriz triangular inferior con unos en la diagonal. De la relación $MA=U$, pre-multiplicando por $M^{-1}$ obtenemos $A = M^{-1} U = L U$. Esta es la **factorización LU** de la matriz $A$. Resolver $A\\vec{x} = \\vec{b}$ se convierte en resolver $L U \\vec{x} = \\vec{b}$. Esto se hace en dos pasos: 1) Resolver $L\\vec{y} = \\vec{b}$ para $\\vec{y}$ (sustitución hacia adelante). 2) Resolver $U\\vec{x} = \\vec{y}$ para $\\vec{x}$ (sustitución hacia atrás).\n",
        "\n",
        "El algoritmo básico para la factorización LU mediante eliminación Gaussiana es:\n",
        "\n",
        "```\n",
        "  for k = 1 to n-1:\n",
        "    if a_kk == 0 -> stop\n",
        "    for i = k+1 to n:\n",
        "      m_ik = a_ik / a_kk\n",
        "    end\n",
        "    for j = k+1 to n:\n",
        "      for i = k+1 to n:\n",
        "        a_ij = a_ij - m_ik * a_kj\n",
        "      end\n",
        "    end\n",
        "  end\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "365a1971-c215-4d06-98c9-5b74f85461de",
      "metadata": {
        "id": "365a1971-c215-4d06-98c9-5b74f85461de"
      },
      "source": [
        "### 1.2 Métodos Iterativos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iterative-methods",
      "metadata": {
        "id": "iterative-methods"
      },
      "source": [
        "Resolver el sistema $A\\vec{x} = \\vec{b}$ calculando $A^{-1}$ es de elevado costo. Los métodos directos como la factorización LU son una alternativa. Sin embargo, para **sistemas grandes**, los costos de memoria y computacionales de los métodos directos pueden ser prohibitivos. En estos casos, o para **sistemas matriciales especiales**, es posible utilizar un **método iterativo que converge asintóticamente a la solución**.\n",
        "\n",
        "Los métodos iterativos presentan una ventaja computacional para:\n",
        "*   Matrices diagonales dominantes (DD) ($|a_{ii}| > \\sum_{j \\neq i} |a_{ij}|$).\n",
        "*   Matrices simétricas y definidas positivas (SDP) ($A^T = A$ y $\\vec{x}^T A \\vec{x} > 0$ para todo $\\vec{x} \\neq \\vec{0}$).\n",
        "*   Matrices sparse (con un número reducido de elementos no nulos).\n",
        "*   Matrices triangulares (superior o inferior).\n",
        "*   Matrices bandwidth ($a_{ij}=0$ si $|i-j| > k$).\n",
        "*   Cuando es fácil estimar una solución inicial.\n",
        "\n",
        "La implementación de métodos numéricos para resolver EDPs transientes a menudo produce sistemas matriciales sparse y simétricos, donde la solución del tiempo pasado puede usarse como una buena aproximación inicial para el tiempo presente.\n",
        "\n",
        "#### Iteración Estacionaria\n",
        "\n",
        "El esquema iterativo más sencillo es el de iteración estacionaria:\n",
        "\n",
        "$$ \\vec{x}^{(k+1)} = T\\vec{x}^{(k)} + \\vec{c} $$\n",
        "\n",
        "Donde la matriz $T$ y el vector $\\vec{c}$ se eligen de forma que un punto fijo de la iteración ($\\vec{x}^* = T\\vec{x}^* + \\vec{c}$) sea la solución del sistema $A\\vec{x} = \\vec{b}$ . Se denomina estacionario porque $T$ y $\\vec{c}$ no dependen de la iteración $k$.\n",
        "\n",
        "La matriz $T$ se construye separando la matriz $A$ en $A = M - N$, donde $M$ es una matriz no singular y fácil de invertir. Entonces, $T = M^{-1} N$ y $\\vec{c} = M^{-1} \\vec{b}$. La iteración se puede escribir como $M \\vec{x}^{(k+1)} = N \\vec{x}^{(k)} + \\vec{b}$, lo que implica resolver un sistema lineal con matriz $M$ en cada iteración. La idea es que $M$ sea similar a $A$ pero fácil de invertir (e.g., diagonal, triangular inferior o superior).\n",
        "\n",
        "La convergencia del esquema iterativo está garantizada si el radio espectral de la matriz Jacobiana asociada $J = \\partial G / \\partial \\vec{x} = T = M^{-1}N$ cumple $\\rho(J) < 1$. A menor radio espectral, mayor convergencia. Hay un balance entre la facilidad de invertir $M$ y el radio espectral. El caso extremo $M=A$ converge en una iteración, pero es computacionalmente inviable si $A$ es grande.\n",
        "\n",
        "#### Método de Jacobi\n",
        "\n",
        "Es un método de iteración estacionaria. Se escoge la matriz $M$ como la diagonal de $A$, $M = D = \\text{diag}(A)$, y $-N$ como el resto de la matriz, $N = -(L + U)$, donde $L$ y $U$ son las partes triangular inferior y superior de $A$ respectivamente. Si los elementos diagonales $a_{ii}$ son no nulos, $D$ es invertible.\n",
        "\n",
        "El esquema iterativo es:\n",
        "\n",
        "$$ \\vec{x}^{(k+1)} = D^{-1} (L + U) \\vec{x}^{(k)} + D^{-1} \\vec{b} $$\n",
        "\n",
        "La convergencia está garantizada si la matriz $A$ es diagonal dominante o simétrica definida positiva.\n",
        "\n",
        "El algoritmo del método de Jacobi es:\n",
        "\n",
        "```\n",
        "  x(0) = ... (aproximación inicial)\n",
        "  res = ||A x(0) - b||_p\n",
        "  k = 0\n",
        "  while (res > tol or k < kmax) do\n",
        "    for i = 1, 2, ..., n do\n",
        "      x(k+1)_i = (b_i - sum_{j=1; j!=i}^{n} a_ij * x(k)_j) / a_ii\n",
        "    end for\n",
        "    res = ||A x(k+1) - b||_p\n",
        "    k += 1\n",
        "  end for\n",
        "```\n",
        "\n",
        "#### Método de Gauss-Seidel\n",
        "\n",
        "Una limitación del método de Jacobi es su lenta convergencia, en parte porque no utiliza la información de la iteración actual tan pronto como está disponible. El método de Gauss-Seidel es una modificación que utiliza la información de la iteración actual para calcular la nueva aproximación.\n",
        "\n",
        "Se escoge la matriz $M$ como la suma de la diagonal y la parte triangular inferior de $A$, $M = L + D$, y $-N$ como la parte triangular superior, $N = -U$.\n",
        "\n",
        "El esquema iterativo es:\n",
        "\n",
        "$$ \\vec{x}^{(k+1)} = (L + D)^{-1} (-U \\vec{x}^{(k)} + \\vec{b}) $$\n",
        "\n",
        "Partiendo de $\\vec{x}^{(0)}$, se aproxima la solución resolviendo un sistema lineal con matriz triangular inferior $(L+D)$. En forma de componentes, la fórmula es:\n",
        "\n",
        "$$ x_i^{(k+1)} = \\frac{b_i - \\sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \\sum_{j=i+1}^{n} a_{ij} x_j^{(k)}}{a_{ii}} $$\n",
        "\n",
        "Observe que para calcular $x_i^{(k+1)}$ se utilizan los valores de $x_j^{(k+1)}$ que ya han sido actualizados en la iteración actual para $j < i$, y los valores de $x_j^{(k)}$ de la iteración anterior para $j > i$.\n",
        "\n",
        "La convergencia está garantizada si la matriz $A$ es simétrica definida positiva.\n",
        "\n",
        "El algoritmo del método de Gauss-Seidel es:\n",
        "\n",
        "```\n",
        "  x = ... (aproximación inicial)\n",
        "  res = ||A x - b||_p\n",
        "  k = 0\n",
        "  while (res > tol or k < kmax) do\n",
        "    for i = 1, 2, ..., n do\n",
        "      x_i = (b_i - sum_{j=1}^{i-1} a_ij * x_j - sum_{j=i+1}^{n} a_ij * x_j) / a_ii\n",
        "    end for\n",
        "    res = ||A x - b||_p\n",
        "    k += 1\n",
        "  end for\n",
        "```\n",
        "\n",
        "#### Método del Gradiente Conjugado (CG - Conjugate Gradient)\n",
        "\n",
        "El método del gradiente conjugado es un método iterativo (aunque por construcción es directo) basado en procesos de optimización. Requiere que la matriz $A$ sea **simétrica definida positiva**.\n",
        "\n",
        "La solución del sistema $A\\vec{x} = \\vec{b}$ minimiza la función cuadrática $\\psi(\\vec{x}) = \\frac{1}{2} \\vec{x}^T A \\vec{x} - \\vec{x}^T \\vec{b}$, ya que su gradiente $\\nabla\\psi(\\vec{x}) = A\\vec{x} - \\vec{b}$ es cero en la solución.\n",
        "\n",
        "La idea es moverse desde un punto inicial $\\vec{x}^{(0)}$ hacia el mínimo mediante pasos discretos en una dirección $\\vec{s}^{(k)}$ una distancia $\\alpha$:\n",
        "\n",
        "$$ \\vec{x}^{(k+1)} = \\vec{x}^{(k)} + \\alpha \\vec{s}^{(k)} $$\n",
        "\n",
        "La distancia $\\alpha$ óptima para cualquier dirección de búsqueda $\\vec{s}^{(k)}$ es aquella que hace que el nuevo vector residuo $\\vec{r}^{(k+1)} = \\vec{b} - A\\vec{x}^{(k+1)}$ sea perpendicular a la dirección de búsqueda $\\vec{s}^{(k)}$: $\\vec{r}^{(k+1)T} \\vec{s}^{(k)} = 0$. El residuo se actualiza como $\\vec{r}^{(k+1)} = \\vec{r}^{(k)} - \\alpha A\\vec{s}^{(k)}$. El paso óptimo $\\alpha$ se calcula como:\n",
        "\n",
        "$$ \\alpha = \\frac{\\vec{r}^{(k)T} \\vec{s}^{(k)}}{\\vec{s}^{(k)T} A \\vec{s}^{(k)}} $$\n",
        "\n",
        "Una elección simple para la dirección de búsqueda sería el negativo del gradiente, $\\vec{s}^{(k)} = -\\nabla\\psi(\\vec{x}^{(k)}) = \\vec{r}^{(k)}$ (método del descenso del gradiente). Sin embargo, esto puede llevar a convergencia sub-óptima. El método del gradiente conjugado utiliza direcciones de búsqueda que son **ortogonales respecto a la matriz A (A-conjugadas)**: $\\vec{s}^{(k+1)T} A \\vec{s}^{(k)} = 0$.\n",
        "\n",
        "El algoritmo del método de gradiente conjugado es:\n",
        "\n",
        "```\n",
        "  x(0) = ... (aproximación inicial)\n",
        "  r(0) = b - A * x(0)\n",
        "  s(0) = r(0)\n",
        "  k = 0\n",
        "  while (||r(k)||_p > tol or k < kmax) do\n",
        "    alpha = (r(k)' * r(k)) / (s(k)' * A * s(k))\n",
        "    x(k+1) = x(k) + alpha * s(k)\n",
        "    r(k+1) = r(k) - alpha * A * s(k)\n",
        "    beta = (r(k+1)' * r(k+1)) / (r(k)' * r(k))\n",
        "    s(k+1) = r(k+1) + beta * s(k)\n",
        "    k += 1\n",
        "  end while\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "placeholder-lu",
      "metadata": {
        "id": "placeholder-lu"
      },
      "outputs": [],
      "source": [
        "### def SolveEliminacionGauss(A_, b_):\n",
        "###     # Implementación de eliminacion Gaussiana\n",
        "###     return xHat\n",
        "import numpy as np\n",
        "\n",
        "def SolveEliminacionGauss(A_, b_):\n",
        "    n = len(b_)\n",
        "    A = A_.copy().astype(float)\n",
        "    b = b_.copy().astype(float)\n",
        "\n",
        "    # Eliminación hacia adelante\n",
        "    for k in range(n-1):\n",
        "        max_row = np.argmax(np.abs(A[k:, k])) + k\n",
        "        if max_row != k:\n",
        "            A[[k, max_row]] = A[[max_row, k]]\n",
        "            b[[k, max_row]] = b[[max_row, k]]\n",
        "\n",
        "        if A[k, k] == 0:\n",
        "            raise ValueError(\"Matriz singular detectada durante la eliminación\")\n",
        "\n",
        "        for i in range(k+1, n):\n",
        "            factor = A[i, k] / A[k, k]\n",
        "            A[i, k:] -= factor * A[k, k:]\n",
        "            b[i] -= factor * b[k]\n",
        "\n",
        "    # Sustitución hacia atrás\n",
        "    xHat = np.zeros(n)\n",
        "    for i in range(n-1, -1, -1):\n",
        "        if A[i, i] == 0:\n",
        "            raise ValueError(\"Matriz singular detectada durante la sustitución\")\n",
        "        xHat[i] = (b[i] - np.dot(A[i, i+1:], xHat[i+1:])) / A[i, i]\n",
        "\n",
        "    return xHat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "placeholder-jacobi",
      "metadata": {
        "id": "placeholder-jacobi"
      },
      "outputs": [],
      "source": [
        "### def SolveJacobi(A_, b_, x0_, tol_=1e-6, kmax_=1000):\n",
        "###     # Implementación del Método de Jacobi\n",
        "###     return xHat, k\n",
        "def SolveJacobi(A_, b_, x0_, tol_=1e-6, kmax_=1000):\n",
        "    n = len(b_)\n",
        "    xHat = x0_.copy().astype(float)\n",
        "    x_new = np.zeros_like(xHat)\n",
        "    k = 0\n",
        "    res = np.linalg.norm(A_ @ xHat - b_, 2)\n",
        "\n",
        "    while res > tol_ and k < kmax_:\n",
        "        for i in range(n):\n",
        "            sigma = np.dot(A_[i, :i], xHat[:i]) + np.dot(A_[i, i+1:], xHat[i+1:])\n",
        "            x_new[i] = (b_[i] - sigma) / A_[i, i]\n",
        "\n",
        "        xHat = x_new.copy()\n",
        "        res = np.linalg.norm(A_ @ xHat - b_, 2)\n",
        "        k += 1\n",
        "\n",
        "    return xHat, k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "placeholder-gauss-seidel",
      "metadata": {
        "id": "placeholder-gauss-seidel"
      },
      "outputs": [],
      "source": [
        "### def SolveGaussSeidel(A_, b_, x0_, tol_=1e-6, kmax_=1000):\n",
        "###     # Implementación del Método de Gauss-Seidel\n",
        "###     return xHat, k\n",
        "def SolveGaussSeidel(A_, b_, x0_, tol_=1e-6, kmax_=1000):\n",
        "    n = len(b_)\n",
        "    xHat = x0_.copy().astype(float)\n",
        "    k = 0\n",
        "    res = np.linalg.norm(A_ @ xHat - b_, 2)\n",
        "\n",
        "    while res > tol_ and k < kmax_:\n",
        "        for i in range(n):\n",
        "            sigma = np.dot(A_[i, :i], xHat[:i]) + np.dot(A_[i, i+1:], xHat[i+1:])\n",
        "            xHat[i] = (b_[i] - sigma) / A_[i, i]\n",
        "\n",
        "        res = np.linalg.norm(A_ @ xHat - b_, 2)\n",
        "        k += 1\n",
        "\n",
        "    return xHat, k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "placeholder-cg",
      "metadata": {
        "id": "placeholder-cg"
      },
      "outputs": [],
      "source": [
        "### def SolveCG(A_, b_, x0_, tol_=1e-6, kmax_=None):\n",
        "###     # Implementación del Método de Gradiente Conjugado\n",
        "###     # Requiere que A sea Simétrica Definida Positiva\n",
        "###     return xHat, k\n",
        "def SolveCG(A_, b_, x0_, tol_=1e-6, kmax_=None):\n",
        "    n = len(b_)\n",
        "    if kmax_ is None:\n",
        "        kmax_ = n\n",
        "\n",
        "    xHat = x0_.copy().astype(float)\n",
        "    r = b_ - A_ @ xHat\n",
        "    s = r.copy()\n",
        "    k = 0\n",
        "    res = np.linalg.norm(r, 2)\n",
        "\n",
        "    while res > tol_ and k < kmax_:\n",
        "        alpha = np.dot(r, r) / np.dot(s, A_ @ s)\n",
        "        xHat = xHat + alpha * s\n",
        "        rn = r - alpha * (A_ @ s)\n",
        "        beta = np.dot(rn, rn) / np.dot(r, r)\n",
        "        s = rn + beta * s\n",
        "        r = rn\n",
        "        res = np.linalg.norm(r, 2)\n",
        "        k += 1\n",
        "\n",
        "    return xHat, k"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "exercises",
      "metadata": {
        "id": "exercises"
      },
      "source": [
        "## 2. Ejercicios propuestos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "exercise-description",
      "metadata": {
        "id": "exercise-description"
      },
      "source": [
        "### Distribución de Temperaturas en una Barra (Conducción 1D en Estado Estacionario)\n",
        "\n",
        "Considere una barra metálica de longitud $L$ con área de sección transversal constante $A_c$, conductividad térmica $k$. La barra está aislada en sus superficies laterales y sufre convección con el ambiente a temperatura $T_\\infty$ con un coeficiente de convección $h$. Los extremos de la barra se mantienen a temperaturas fijas $T_0$ en $x=0$ y $T_L$ en $x=L$.\n",
        "\n",
        "![image](https://github.com/nicolas-thiers/Mec-270-Lab/blob/main/Lab05/Images/Esquema-1.svg?raw=1)\n",
        "\n",
        "La ecuación de gobierno para la distribución de temperatura $T(x)$ en estado estacionario es:\n",
        "\n",
        "$$ \\frac{d}{dx} \\left( k \\frac{dT}{dx} \\right) - \\frac{h P}{A_c} (T - T_\\infty) = 0 $$\n",
        "\n",
        "donde $P$ es el perímetro de la sección transversal.\n",
        "\n",
        "Utilizaremos el método de diferencias finitas para discretizar esta ecuación en $N$ nodos espaciados uniformemente a lo largo de la barra ($x_i = i \\Delta x$, donde $\\Delta x = L/(N-1)$). Aproximando las derivadas segundas:\n",
        "\n",
        "$$ \\frac{d^2T}{dx^2} \\approx \\frac{T_{i-1} - 2T_i + T_{i+1}}{(\\Delta x)^2} $$\n",
        "\n",
        "Asumiendo conductividad $k$ constante y definiendo $m^2 = h P / (k A_c)$, la ecuación discretizada en el nodo $i$ (para $i=1, \\dots, N-2$) es:\n",
        "\n",
        "$$ k \\frac{T_{i-1} - 2T_i + T_{i+1}}{(\\Delta x)^2} - m^2 k (T_i - T_\\infty) = 0 $$\n",
        "\n",
        "Reordenando, obtenemos una ecuación lineal para cada nodo interior $i$:\n",
        "\n",
        "$$ T_{i-1} - (2 + m^2 (\\Delta x)^2) T_i + T_{i+1} = -m^2 (\\Delta x)^2 T_\\infty $$\n",
        "\n",
        "Para los nodos en los extremos ($i=0$ e $i=N-1$), las condiciones de frontera son simplemente $T_0$ y $T_{N-1} = T_L$.\n",
        "\n",
        "Esto nos lleva a un sistema de $N$ ecuaciones lineales con $N$ incógnitas ($T_0, T_1, \\dots, T_{N-1}$) de la forma $A\\vec{T} = \\vec{b}$. Para $N=5$ nodos, el sistema se ve así:\n",
        "\n",
        "$$i=0: T_0 = T_{\\text{frontera},0}$$\n",
        "\n",
        "$$i=1: T_0 - (2 + m^2 (\\Delta x)^2) T_1 + T_2 = -m^2 (\\Delta x)^2 T_\\infty$$\n",
        "\n",
        "$$i=2: T_1 - (2 + m^2 (\\Delta x)^2) T_2 + T_3 = -m^2 (\\Delta x)^2 T_\\infty$$\n",
        "\n",
        "$$i=3: T_2 - (2 + m^2 (\\Delta x)^2) T_3 + T_4 = -m^2 (\\Delta x)^2 T_\\infty$$\n",
        "\n",
        "$$i=4: T_4 = T_{\\text{frontera},L}$$\n",
        "\n",
        "Sustituyendo las condiciones de frontera en las ecuaciones para $i=1$ y $i=N-2=3$:\n",
        "\n",
        "$$i=1: - (2 + m^2 (\\Delta x)^2) T_1 + T_2 = -m^2 (\\Delta x)^2 T_\\infty - T_{\\text{frontera},0}$$\n",
        "\n",
        "$$i=3: T_2 - (2 + m^2 (\\Delta x)^2) T_3 = -m^2 (\\Delta x)^2 T_\\infty - T_{\\text{frontera},L}$$\n",
        "\n",
        "El sistema para los $N-2$ nodos interiores ($T_1, T_2, T_3$) es:\n",
        "\n",
        "$$\\begin{bmatrix} -(2 + m^2 (\\Delta x)^2) & 1 & 0 \\\\ 1 & -(2 + m^2 (\\Delta x)^2) & 1 \\\\ 0 & 1 & -(2 + m^2 (\\Delta x)^2) \\end{bmatrix} \\begin{bmatrix} T_1 \\\\ T_2 \\\\ T_3 \\end{bmatrix} = \\begin{bmatrix} -m^2 (\\Delta x)^2 T_\\infty - T_{\\text{frontera},0} \\\\ -m^2 (\\Delta x)^2 T_\\infty \\\\ -m^2 (\\Delta x)^2 T_\\infty - T_{\\text{frontera},L} \\end{bmatrix}$$\n",
        "\n",
        "Defina $C = 2 + m^2 (\\Delta x)^2$ y $R = -m^2 (\\Delta x)^2 T_\\infty$. El sistema es:\n",
        "\n",
        "$$ \\begin{bmatrix} -C & 1 & 0 \\\\ 1 & -C & 1 \\\\ 0 & 1 & -C \\end{bmatrix} \\begin{bmatrix} T_1 \\\\ T_2 \\\\ T_3 \\end{bmatrix} = \\begin{bmatrix} R - T_{\\text{frontera},0} \\\\ R \\\\ R - T_{\\text{frontera},L} \\end{bmatrix} $$\n",
        "\n",
        "Este es un sistema de 3x3 para los nodos interiores. La matriz del sistema es **tri-diagonal**, lo que la hace un tipo de matriz especial. Para un número mayor de nodos $N$, la matriz del sistema para los $N-2$ nodos interiores será de tamaño $(N-2) \\times (N-2)$ y también tri-diagonal.\n",
        "\n",
        "**Datos:**\n",
        "\n",
        "1.  Longitud de la barra $L = 1$ m\n",
        "2.  Parámetro $m^2 = 10$ m$^{-2}$\n",
        "3.  Temperatura ambiente $T_\\infty = 20$ °C\n",
        "4.  Temperatura en $x=0$, $T_0 = 100$ °C\n",
        "5.  Temperatura en $x=L$, $T_L = 50$ °C\n",
        "6.  Número de nodos $N=10$\n",
        "7.  Tolerancia para métodos iterativos $tol = 10^{-6}$\n",
        "8.  Máximo de iteraciones para métodos iterativos $k_{max} = 1000$\n",
        "\n",
        "**Tareas:**\n",
        "\n",
        "1.  Calcule el tamaño de la matriz del sistema lineal para los nodos interiores ($N-2$) y el valor de $\\Delta x$.\n",
        "2.  Construya la matriz $A$ y el vector $\\vec{b}$ para el sistema de $N-2$ ecuaciones lineales para los nodos interiores.\n",
        "3.  Resuelva el sistema utilizando un **método directo**. Reporte las temperaturas $T_1, T_2, \\dots, T_{N-2}$.\n",
        "4.  Resuelva el sistema utilizando los **métodos iterativos** de Jacobi, Gauss-Seidel . Use un vector inicial de ceros (o todos unos). Reporte las temperaturas $T_1, \\dots, T_{N-2}$ y el número de iteraciones necesarias para alcanzar la tolerancia $tol$. Calcule el residuo final $||A\\hat{x} - \\vec{b}||_p$.\n",
        "5.  Resuelva el sistema utilizando el **método del Gradiente Conjugado**. Asegúrese de que la matriz $A$ sea simétrica definida positiva para aplicar este método. Reporte las temperaturas y el número de iteraciones. Compare la convergencia (respecto al número de iteraciones) con Jacobi y Gauss-Seidel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "exercise-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exercise-code",
        "outputId": "73338368-07d8-41bd-90df-f544c6cad1ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño de la matriz del sistema: 8x8\n",
            "Delta_x = 0.1111 m\n",
            "\n",
            "Matriz A:\n",
            "[[-2.12345679  1.          0.          0.          0.          0.\n",
            "   0.          0.        ]\n",
            " [ 1.         -2.12345679  1.          0.          0.          0.\n",
            "   0.          0.        ]\n",
            " [ 0.          1.         -2.12345679  1.          0.          0.\n",
            "   0.          0.        ]\n",
            " [ 0.          0.          1.         -2.12345679  1.          0.\n",
            "   0.          0.        ]\n",
            " [ 0.          0.          0.          1.         -2.12345679  1.\n",
            "   0.          0.        ]\n",
            " [ 0.          0.          0.          0.          1.         -2.12345679\n",
            "   1.          0.        ]\n",
            " [ 0.          0.          0.          0.          0.          1.\n",
            "  -2.12345679  1.        ]\n",
            " [ 0.          0.          0.          0.          0.          0.\n",
            "   1.         -2.12345679]]\n",
            "\n",
            "Vector b:\n",
            "[-102.4691358   -2.4691358   -2.4691358   -2.4691358   -2.4691358\n",
            "   -2.4691358   -2.4691358  -52.4691358]\n",
            "\n",
            "Solución por método directo (Eliminación Gaussiana):\n",
            "[77.21525324 61.49411799 50.89571336 44.11159433 40.30421534 39.0035296\n",
            " 40.04895862 43.56956772]\n",
            "\n",
            "Solución por Jacobi (142 iteraciones, residual=9.39e-07):\n",
            "[77.21525263 61.49411688 50.89571182 44.11159263 40.30421358 39.0035281\n",
            " 40.04895748 43.56956713]\n",
            "\n",
            "Solución por Gauss-Seidel (72 iteraciones, residual=8.33e-07):\n",
            "[77.2152525  61.49411677 50.8957119  44.11159286 40.30421403 39.00352858\n",
            " 40.04895796 43.56956741]\n",
            "\n",
            "La matriz A es simétrica\n",
            "La matriz A no es definida positiva (no se puede aplicar CG)\n",
            "\n",
            "Comparación de resultados:\n",
            "Eliminación Gaussiana: [77.21525324 61.49411799 50.89571336 44.11159433 40.30421534 39.0035296\n",
            " 40.04895862 43.56956772]\n",
            "Jacobi:              [77.21525263 61.49411688 50.89571182 44.11159263 40.30421358 39.0035281\n",
            " 40.04895748 43.56956713]\n",
            "Gauss-Seidel:        [77.2152525  61.49411677 50.8957119  44.11159286 40.30421403 39.00352858\n",
            " 40.04895796 43.56956741]\n",
            "\n",
            "Comparación de convergencia (número de iteraciones):\n",
            "Jacobi: 142\n",
            "Gauss-Seidel: 72\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#Manuel Casanova Casanova\n",
        "# Parámetros del problema\n",
        "L = 1.0          # Longitud de la barra (m)\n",
        "m2 = 10.0        # Parámetro m^2 (m^-2)\n",
        "T_inf = 20.0     # Temperatura ambiente (°C)\n",
        "T0 = 100.0       # Temperatura en x=0 (°C)\n",
        "TL = 50.0        # Temperatura en x=L (°C)\n",
        "N = 10           # Número de nodos\n",
        "tol = 1e-6       # Tolerancia para métodos iterativos\n",
        "kmax = 1000      # Máximo de iteraciones\n",
        "\n",
        "# 1. Calcular tamaño de la matriz y Δx\n",
        "Delta_x = L / (N - 1)\n",
        "nodos_interiores = N - 2\n",
        "print(f\"Tamaño de la matriz del sistema: {nodos_interiores}x{nodos_interiores}\")\n",
        "print(f\"Delta_x = {Delta_x:.4f} m\")\n",
        "\n",
        "# 2. Construir la matriz A y el vector b\n",
        "C = 2 + m2 * Delta_x**2\n",
        "R = -m2 * Delta_x**2 * T_inf\n",
        "\n",
        "# Inicializar matriz A (tri-diagonal)\n",
        "A = np.zeros((nodos_interiores, nodos_interiores))\n",
        "for i in range(nodos_interiores):\n",
        "    A[i, i] = -C\n",
        "    if i > 0:\n",
        "        A[i, i-1] = 1\n",
        "    if i < nodos_interiores - 1:\n",
        "        A[i, i+1] = 1\n",
        "\n",
        "# Construir vector b\n",
        "b = np.full(nodos_interiores, R)\n",
        "b[0] -= T0\n",
        "b[-1] -= TL\n",
        "\n",
        "print(\"\\nMatriz A:\")\n",
        "print(A)\n",
        "print(\"\\nVector b:\")\n",
        "print(b)\n",
        "\n",
        "# 3. Resolver usando método directo (Eliminación Gaussiana)\n",
        "T_direct = SolveEliminacionGauss(A, b)\n",
        "print(\"\\nSolución por método directo (Eliminación Gaussiana):\")\n",
        "print(T_direct)\n",
        "\n",
        "# 4. Resolver usando métodos iterativos\n",
        "# Vector inicial (ceros o unos)\n",
        "x0 = np.zeros(nodos_interiores)\n",
        "\n",
        "# Método de Jacobi\n",
        "T_jacobi, iter_jacobi = SolveJacobi(A, b, x0, tol, kmax)\n",
        "residual_jacobi = np.linalg.norm(A @ T_jacobi - b, 2)\n",
        "print(f\"\\nSolución por Jacobi ({iter_jacobi} iteraciones, residual={residual_jacobi:.2e}):\")\n",
        "print(T_jacobi)\n",
        "\n",
        "# Método de Gauss-Seidel\n",
        "T_gs, iter_gs = SolveGaussSeidel(A, b, x0, tol, kmax)\n",
        "residual_gs = np.linalg.norm(A @ T_gs - b, 2)\n",
        "print(f\"\\nSolución por Gauss-Seidel ({iter_gs} iteraciones, residual={residual_gs:.2e}):\")\n",
        "print(T_gs)\n",
        "\n",
        "# 5. Resolver usando Gradiente Conjugado\n",
        "# Verificar si la matriz es simétrica definida positiva\n",
        "if np.allclose(A, A.T):\n",
        "    print(\"\\nLa matriz A es simétrica\")\n",
        "    # Verificar si es definida positiva (todos los autovalores > 0)\n",
        "    eigenvalues = np.linalg.eigvals(A)\n",
        "    if np.all(eigenvalues > 0):\n",
        "        print(\"La matriz A es definida positiva\")\n",
        "\n",
        "        T_cg, iter_cg = SolveCG(A, b, x0, tol, kmax)\n",
        "        residual_cg = np.linalg.norm(A @ T_cg - b, 2)\n",
        "        print(f\"\\nSolución por Gradiente Conjugado ({iter_cg} iteraciones, residual={residual_cg:.2e}):\")\n",
        "        print(T_cg)\n",
        "    else:\n",
        "        print(\"La matriz A no es definida positiva (no se puede aplicar CG)\")\n",
        "else:\n",
        "    print(\"\\nLa matriz A no es simétrica (no se puede aplicar CG)\")\n",
        "\n",
        "# Comparación de resultados\n",
        "print(\"\\nComparación de resultados:\")\n",
        "print(f\"Eliminación Gaussiana: {T_direct}\")\n",
        "print(f\"Jacobi:              {T_jacobi}\")\n",
        "print(f\"Gauss-Seidel:        {T_gs}\")\n",
        "if 'T_cg' in locals():\n",
        "    print(f\"Gradiente Conjugado: {T_cg}\")\n",
        "\n",
        "# Comparación de convergencia\n",
        "print(\"\\nComparación de convergencia (número de iteraciones):\")\n",
        "print(f\"Jacobi: {iter_jacobi}\")\n",
        "print(f\"Gauss-Seidel: {iter_gs}\")\n",
        "if 'iter_cg' in locals():\n",
        "    print(f\"Gradiente Conjugado: {iter_cg}\")\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}